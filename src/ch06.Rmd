---
title: "Chapter 6"
author: "Matt Emery"
date: "December 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```


## Easy.

**6E1.** State the three motivating criteria that define information entropy. Try to express each in your own words.

1. The function must be continuous, meaning that a small change can't cause an infinite increase.
2. The function should increase as the number of possibilities increase.
3. It must be additive. This means you should be able to combine entropies together.

**6E2.** Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

```{r}
info_entropy <- function(probabilities) {
  -sum(purrr::map_dbl(probabilities, ~ .*log(.)))
}
```


```{r}
info_entropy(c(0.7, 0.3))
```

**6E3.** Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?

```{r}
info_entropy(c(0.2, 0.25, 0.25, 0.3))
```

**6E4.** Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?

If an event never occurs, it's not considered

```{r}
info_entropy(c(1/3, 1/3, 1/3))
```

## Medium.

**6M1.** Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?

$$\mathrm { AIC } = D _ { \mathrm { train } } + 2 p$$

D_train is the deviance in the predictions on the training set and p is the number of parameters in the model. 

DIC is more broad than AIC.

$$\mathrm { DIC } = \overline { D } + ( \overline { D } - \hat { D } ) = \overline { D } + p _ { D }$$

Now deviance is a distribution. $\overline { D }$ is the average of the distribution. $\hat { D }$ is the average of each parameter in the posterior distribution. $p _ { D }$ is the difference between the two and represents the effective number of parameters in a model. When the priors are flat, $p _ { D }$ becomes $2p$.

WAIC is the most broad of the three.

$$\operatorname { lppd } = \sum _ { i = 1 } ^ { N } \log \operatorname { Pr } \left( y _ { i } \right)$$


$$p _ { \mathrm { WAIC } } = \sum _ { i = 1 } ^ { N } V \left( y _ { i } \right)$$

$$\mathrm { WAIC } = - 2 \left( \operatorname { lppd } - p _ { \mathrm { WAIC } } \right)$$

WAIC is calculated pointwise. lppd is the sum of average log-likelihood for each point in the training sample. p_WAIC is the sum of variances in log-likelihood for each item. WAIC no longer requires a Gaussian posterior and can accept hierarchical models. WAIC tends towards DIC as the amount of data overwhelms the prior and the deviance can be adequately described by the mean of the point devfiances.

**6M2.** Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging?

Model selection means choosing a model based on some information criteria (or cross-validation). Model averaging involves creating an ensemble weighted by the difference in informatian criteria. Using model selection means removing information about the IC distances of models. Model averaging combines uncertainties, so information about individual uncertainities in the ensemble prediction are lost.

**6M3.** When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.

```{r}
set.seed(0)
dataset <- matrix(rnorm(n = 3000, mean = 0, sd = 1), nrow = 1000)
target <- rowSums(dataset) * rnorm(100)
df <- as_data_frame(dataset) %>% 
  mutate(target = target) %>% 
  as.data.frame()
```

```{r}
model <- alist(
  target ~ dnorm(mu, sigma),
  mu <- a + b1 * V1 + b2 * V2 + b3 * V3,
  a ~ dnorm(0, 5),
  b1 ~ dnorm(0, 1),
  b2 ~ dnorm(0, 1),
  b3 ~ dnorm(0, 1),
  sigma ~ dunif(0, 50)
               )
```

```{r}
fit_model <- rethinking::map(model, data = df)
rethinking::WAIC(fit_model)[[1]]
```

```{r}
fit_model <- rethinking::map(model, data = df[1:100,])
rethinking::WAIC(fit_model)[[1]]
```

WAIC is 10 times lower for when there is 10 times less data. If we believed that we could compare between datasets we would choose to have less data.

**6M4.** What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

```{r}
fit_model <- rethinking::map(model, data = df[1:100,])
attr(rethinking::WAIC(fit_model), "pWAIC")
```

```{r}
tight_prior_model <- alist(
  target ~ dnorm(mu, sigma),
  mu <- a + b1 * V1 + b2 * V2 + b3 * V3,
  a ~ dnorm(0, 0.5),
  b1 ~ dnorm(0, 0.1),
  b2 ~ dnorm(0, 0.1),
  b3 ~ dnorm(0, 0.1),
  sigma ~ dunif(0, 5)
               )
fit_model <- rethinking::map(tight_prior_model, data = df[1:100,])
attr(rethinking::WAIC(fit_model), "pWAIC")
```

```{r}
wide_prior_model <- alist(
  target ~ dnorm(mu, sigma),
  mu <- a + b1 * V1 + b2 * V2 + b3 * V3,
  a ~ dnorm(0, 50),
  b1 ~ dnorm(0, 10),
  b2 ~ dnorm(0, 10),
  b3 ~ dnorm(0, 10),
  sigma ~ dunif(0, 500)
               )
fit_model <- rethinking::map(wide_prior_model, data = df[1:100,])
attr(rethinking::WAIC(fit_model), "pWAIC")
```

In the case of tight priors, WAIC is lower. This is because the number of effective parameters is also lower. 

**6M5.** Provide an informal explanation of why informative priors reduce overfitting.

Informative priors act as "previous data" to the model. If you have informative priors, a lot of data is required to change the parameters.

**6M6.** Provide an information explanation of why overly informative priors result in underfitting.

If the "true" parameter is far from the center of the prior and the prior is very informative, the there may not be enough data to overwhelm the prior.

## Hard.

All practice problems to follow use the same data. Pull out the old Howell !Kung demography data and split it into two equally sized data frames. Here’s the code to do it:

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d$age <- (d$age - mean(d$age))/sd(d$age)
set.seed( 1000 )
i <- sample(1:nrow(d), size = nrow(d)/2)
d1 <- d[ i , ]
d2 <- d[ -i , ]
```

You now have two randomly formed data frames, each with 272 rows. The notion here is to use the cases in d1 to fit models and the cases in d2 to evaluate them. The set.seed command just ensures that everyone works with the same randomly shuffled data. Now let $h_i$ and $x_i$ be the height and centered age values, respectively, on row i. Fit the following models to the data in d1:

$$\mathcal { M } _ { 1 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i }$$
$$\mathcal { M } _ { 2 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 }$$
$$\mathcal { M } _ { 3 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 } + \beta _ { 3 } x _ { i } ^ { 3 }$$
$$\mathcal { M } _ { 4 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 } + \beta _ { 3 } x _ { i } ^ { 3 } + \beta _ { 4 } x _ { i } ^ { 4 }$$
$$\mathcal { M } _ { 5 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 } + \beta _ { 3 } x _ { i } ^ { 3 } + \beta _ { 4 } x _ { i } ^ { 4 } + \beta _ { 5 } x _ { i } ^ { 5 }$$
$$\mathcal { M } _ { 6 } : h _ { i } \sim \mathrm { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 } + \beta _ { 3 } x _ { i } ^ { 3 } + \beta _ { 4 } x _ { i } ^ { 4 } + \beta _ { 5 } x _ { i } ^ { 5 } + \beta _ { 6 } x _ { i } ^ { 6 }$$

Use map to fit these. Use weakly regularizing priors for all parameters.

Note that fitting all of these polynomials to the height-by-age relationship is not a good way to derive insight. It would be better to have a simpler approach that would allow for more insight, like perhaps a piecewise linear model. But the set of polynomial families above will serve to help you practice and understand model comparison and averaging.

```{r}
first_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_first_model <- rethinking::map(first_model, data = d1)

second_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age + b_2 * age ^ 2,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  b_2 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_second_model <- rethinking::map(second_model, data = d1)

third_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age + b_2 * age ^ 2 + b_3 * age ^ 3,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  b_2 ~ dnorm(0, 5),
  b_3 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_third_model <- rethinking::map(third_model, data = d1)

fourth_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age + b_2 * age ^ 2 + b_3 * age ^ 3 + b_4 * age ^ 4,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  b_2 ~ dnorm(0, 5),
  b_3 ~ dnorm(0, 5),
  b_4 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_fourth_model <- rethinking::map(fourth_model, data = d1)

fifth_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age + b_2 * age ^ 2 + b_3 * age ^ 3 + b_4 * age ^ 4 + b_5 * age ^ 5,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  b_2 ~ dnorm(0, 5),
  b_3 ~ dnorm(0, 5),
  b_4 ~ dnorm(0, 5),
  b_5 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_fifth_model <- rethinking::map(fifth_model, data = d1)

sixth_model <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b_1 * age + b_2 * age ^ 2 + b_3 * age ^ 3 + b_4 * age ^ 4 + b_5 * age ^ 5 + b_6 * age ^ 6,
  a ~ dnorm(0, 5),
  b_1 ~ dnorm(0, 5),
  b_2 ~ dnorm(0, 5),
  b_3 ~ dnorm(0, 5),
  b_4 ~ dnorm(0, 5),
  b_5 ~ dnorm(0, 5),
  b_6 ~ dnorm(0, 5),
  sigma ~ dunif(0, 25)
)
fit_sixth_model <- rethinking::map(sixth_model, data = d1)
```

**6H1.** Compare the models above, using WAIC. Compare the model rankings, as well as the WAIC weights.

```{r}
rethinking::compare(fit_first_model, fit_second_model, fit_third_model, fit_fourth_model, fit_fifth_model, fit_sixth_model, func = WAIC)
```

The fourth model was found to me the "best." 

**6H2.** For each model, produce a plot with model averaged mean and 97% confidence interval of the mean, superimposed on the raw data. How do predictions differ across models?

**6H3.** Now also plot the model averaged predictions, across all models. In what ways do the averaged predictions differ from the predictions of the model with the lowest WAIC value?

**6H4.** Compute the test-sample deviance for each model. This means calculating deviance, but using the data in d2 now. You can compute the log-likelihood of the height data with:

```{r}
sum( dnorm( d2$height , mu , sigma , log=TRUE ) )
```

where mu is a vector of predicted means (based upon age values and MAP parameters) and sigma is the MAP standard deviation.

**6H5.** Compare the deviances from **6H4** to the WAIC values. It might be easier to compare if you subtract the smallest value in each list from the others. For example, subtract the minimum WAIC from all of the WAIC values so that the best WAIC is normalized to zero. Which model makes the best out-of-sample predictions in this case? Does WAIC do a good job of estimating the test deviance?

$$h _ { i } \sim \operatorname { Normal } \left( \mu _ { i } , \sigma \right)$$
$$\mu _ { i } = \alpha + \beta _ { 1 } x _ { i } + \beta _ { 2 } x _ { i } ^ { 2 } + \beta _ { 3 } x _ { i } ^ { 3 } + \beta _ { 4 } x _ { i } ^ { 4 } + \beta _ { 5 } x _ { i } ^ { 5 } + \beta _ { 6 } x _ { i } ^ { 6 }$$
$$\beta _ { 1 } \sim \operatorname { Normal } ( 0,5 )$$
$$\beta _ { 2 } \sim \operatorname { Normal } ( 0,5 )$$
$$\beta _ { 3 } \sim \operatorname { Normal } ( 0,5 )$$
$$\beta _ { 4 } \sim \operatorname { Normal } ( 0,5 )$$
$$\beta _ { 5 } \sim \operatorname { Normal } ( 0,5 )$$
$$\beta _ { 6 } \sim \operatorname { Normal } ( 0,5 )$$

and assume flat (or nearly flat) priors on α and σ. This model contains more strongly regularizing priors on the coefficients.

First, fit this model to the data in d1 . Report the MAP estimates and plot the implied predictions. Then compute the out-of-sample deviance using the data in d2 , using MAP estimates from the model fit to d1 only. How does this model, using regularizing priors, compare to the best WAIC model from earlier? How do you interpret this result?